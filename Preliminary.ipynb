{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f25849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import pearsonr, shapiro, levene\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, f1_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, validation_curve, learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "# Set seaborn visual style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fd1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, columns=None, method=\"standard\", scaler_all=False):\n",
    "    \"\"\"\n",
    "    Scale the specified columns of the dataframe using the chosen method.\n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe.\n",
    "    columns (list): Columns to be scaled.\n",
    "    method (str): Scaling method (\"standard\", \"minmax\", \"robust\", or \"log\").\n",
    "    scaler_all (bool): If True, scale all columns in the dataframe.\n",
    "    Returns:\n",
    "    DataFrame: Scaled dataframe.\n",
    "    \"\"\"\n",
    "    if columns is None and not scaler_all:\n",
    "        raise ValueError(\"Specify the columns to scale or enable the 'scaler_all' option to scale all columns.\")\n",
    "    \n",
    "    if scaler_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif method == \"log\":\n",
    "        def log_scaler(data):\n",
    "            return np.log1p(data)\n",
    "        scaler = log_scaler\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Possible values are 'standard', 'minmax', 'robust', and 'log'.\")\n",
    "    \n",
    "    if method == \"log\":\n",
    "        df_scaled = df[columns].apply(scaler)\n",
    "    else:\n",
    "        scaler.fit(df[columns])\n",
    "        scaled_data = scaler.transform(df[columns])\n",
    "        df_scaled = pd.DataFrame(scaled_data, columns=columns, index=df.index)\n",
    "    \n",
    "    df = pd.concat([df.drop(columns, axis=1), df_scaled], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f3338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(data, annot=True):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of Pearson correlation.\n",
    "    Parameters:\n",
    "    data (DataFrame): Input dataframe.\n",
    "    annot (bool): If True, display values on the heatmap.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(\"Pearson Correlation Heatmap\")\n",
    "    sns.heatmap(data.corr(), annot=annot, cmap=\"coolwarm\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ac613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected(data, response):\n",
    "    \"\"\"\n",
    "    Perform backward feature selection using OLS regression.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): Input dataframe containing both response and predictor variables.\n",
    "        response (str): Name of the response variable.\n",
    "        \n",
    "    Returns:\n",
    "        model (OLS): Final OLS model after backward feature selection.\n",
    "    \"\"\"\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "    \n",
    "    while remaining and cond:\n",
    "        formula = f\"{response} ~ {' + '.join(remaining)} + 1\"\n",
    "        print(\"--\")\n",
    "        print(formula)\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        score = model.pvalues[1:]\n",
    "        to_remove = score[score == score.max()]\n",
    "        if to_remove.values > 0.05:\n",
    "            print(f\"Remove {to_remove.index[0]} (p-value: {round(to_remove.values[0], 3)})\")\n",
    "            remaining.remove(to_remove.index[0])\n",
    "        else:\n",
    "            cond = False\n",
    "            print(\"Final model selected!\")\n",
    "        print(\"\")\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa4046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected_logistic(data, response):\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    \n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    \n",
    "    cond = True\n",
    "    \n",
    "    while remaining and cond:\n",
    "        formula = \"{} ~ {} + 1\".format(response, \" + \".join(remaining))\n",
    "        print(\"--\")\n",
    "        print(formula)\n",
    "        \n",
    "        # Fit logistic regression model\n",
    "        model = smf.logit(formula, data).fit()\n",
    "        \n",
    "        # Get p-values\n",
    "        score = model.pvalues[1:]\n",
    "        \n",
    "        # Remove variable with highest p-value if greater than 0.05\n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            print(\"Remove\", toRemove.index[0], \"(p-value:\", round(toRemove.values[0], 3), \")\")\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            cond = False\n",
    "            print(\"Final model selected!\")\n",
    "        \n",
    "        print(\"\")\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5ae557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_elbow(X_train, cluster_max=10):\n",
    "    \"\"\"\n",
    "    Use the elbow method to find the optimal number of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (array-like): Training data.\n",
    "    - cluster_max (int): Maximum number of clusters to consider.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Optimal number of clusters.\n",
    "    \"\"\"\n",
    "    inertia = []\n",
    "    k_list = range(1, cluster_max + 1)\n",
    "    for k in k_list:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X_train)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot the elbow curve\n",
    "    plt.plot(k_list, inertia, marker='o')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the optimal number of clusters using the elbow point\n",
    "    optimal_index = np.argmin(np.diff(inertia)) + 1\n",
    "    optimal_n_clusters = optimal_index + 1\n",
    "    print(\"Optimal number of clusters:\", optimal_n_clusters)\n",
    "    \n",
    "    return optimal_n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845fc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_names):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    cm = ConfusionMatrixDisplay(cf, display_labels=model_names)\n",
    "    cm.plot(cmap=plt.cm.Blues)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38060e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_sizes, train_scores, test_scores, title):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.plot(train_sizes, train_mean, label=\"Training score\", color=\"blue\")\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\"blue\")\n",
    "    plt.plot(train_sizes, test_mean, label=\"Cross-validation score\", color=\"red\")\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color=\"red\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96998881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_range, train_scores, test_scores, param_name, title):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.plot(param_range, train_mean, label=\"Training score\", color=\"blue\")\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\"blue\")\n",
    "    plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"red\")\n",
    "    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color=\"red\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
